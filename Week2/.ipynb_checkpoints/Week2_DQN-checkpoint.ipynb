{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca209422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import pygame\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddc73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model which takes in the state as an input and outputs predicted q values for every possible action\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, state_space, action_space, lr = 0.003):\n",
    "        super().__init__()\n",
    "        # Add your architecture parameters here\n",
    "        # You can use nn.Functional\n",
    "        # Remember that the input is of size batch_size x state_space\n",
    "        # and the output is of size batch_size x action_space (ulta ho sakta hai dekh lo)\n",
    "        # TODO: Add code here\n",
    "        self.hidden_state = 128 \n",
    "        self.state_space = state_space \n",
    "        self.action_space = action_space\n",
    "        self.fc1 = nn.Linear(state_space,self.hidden_state)\n",
    "        self.fc2 = nn.Linear(self.hidden_state,self.hidden_state)\n",
    "        self.fc3 = nn.Linear(self.hidden_state,action_space)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: Complete based on your implementation\n",
    "        a1 = self.relu(self.fc1(input))\n",
    "        a2 = self.relu(self.fc2(a1))\n",
    "        a3 = self.fc3(a2)\n",
    "        return a3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37adca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training neural networks, we split the data into batches.\n",
    "# To improve the training, we need to remove the \"correlation\" between game states\n",
    "# The buffer starts storing states and once it reaches maximum capacity, it replaces\n",
    "# states at random which reduces the correlation.\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe4d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The score after the 0th episode is 9.0 \n",
      " The score after the 1th episode is 9.0 \n",
      " The score after the 2th episode is 9.333333333333334 \n",
      " The score after the 3th episode is 9.0 \n",
      " The score after the 4th episode is 9.2 \n",
      " The score after the 5th episode is 9.0 \n",
      " The score after the 6th episode is 9.0 \n",
      " The score after the 7th episode is 9.125 \n",
      " The score after the 8th episode is 9.0 \n",
      " The score after the 9th episode is 9.3 \n",
      " The score after the 10th episode is 9.363636363636363 \n",
      " The score after the 11th episode is 9.25 \n",
      " The score after the 12th episode is 9.23076923076923 \n",
      " The score after the 13th episode is 9.214285714285714 \n",
      " The score after the 14th episode is 9.266666666666667 \n",
      " The score after the 15th episode is 9.25 \n",
      " The score after the 16th episode is 9.294117647058824 \n",
      " The score after the 17th episode is 9.277777777777779 \n",
      " The score after the 18th episode is 9.368421052631579 \n",
      " The score after the 19th episode is 9.45 \n",
      " The score after the 20th episode is 9.428571428571429 \n",
      " The score after the 21th episode is 9.454545454545455 \n",
      " The score after the 22th episode is 9.478260869565217 \n",
      " The score after the 23th episode is 9.5 \n",
      " The score after the 24th episode is 9.56 \n",
      " The score after the 25th episode is 9.576923076923077 \n",
      " The score after the 26th episode is 9.555555555555555 \n",
      " The score after the 27th episode is 9.5 \n",
      " The score after the 28th episode is 9.482758620689655 \n",
      " The score after the 29th episode is 9.433333333333334 \n",
      " The score after the 30th episode is 9.419354838709678 \n",
      " The score after the 31th episode is 9.4375 \n",
      " The score after the 32th episode is 9.454545454545455 \n",
      " The score after the 33th episode is 9.441176470588236 \n",
      " The score after the 34th episode is 9.457142857142857 \n",
      " The score after the 35th episode is 9.444444444444445 \n",
      " The score after the 36th episode is 9.45945945945946 \n",
      " The score after the 37th episode is 9.473684210526315 \n",
      " The score after the 38th episode is 9.487179487179487 \n",
      " The score after the 39th episode is 9.45 \n",
      " The score after the 40th episode is 9.463414634146341 \n",
      " The score after the 41th episode is 9.476190476190476 \n",
      " The score after the 42th episode is 9.488372093023257 \n",
      " The score after the 43th episode is 9.454545454545455 \n",
      " The score after the 44th episode is 9.466666666666667 \n",
      " The score after the 45th episode is 9.434782608695652 \n",
      " The score after the 46th episode is 9.46808510638298 \n",
      " The score after the 47th episode is 9.541666666666666 \n",
      " The score after the 48th episode is 9.53061224489796 \n",
      " The score after the 49th episode is 9.5 \n",
      " The score after the 50th episode is 9.509803921568627 \n",
      " The score after the 51th episode is 9.48076923076923 \n",
      " The score after the 52th episode is 9.471698113207546 \n",
      " The score after the 53th episode is 9.481481481481481 \n",
      " The score after the 54th episode is 9.472727272727273 \n",
      " The score after the 55th episode is 9.5 \n",
      " The score after the 56th episode is 9.491228070175438 \n",
      " The score after the 57th episode is 9.5 \n",
      " The score after the 58th episode is 9.508474576271187 \n",
      " The score after the 59th episode is 9.516666666666667 \n",
      " The score after the 60th episode is 9.540983606557377 \n",
      " The score after the 61th episode is 9.548387096774194 \n",
      " The score after the 62th episode is 9.53968253968254 \n",
      " The score after the 63th episode is 9.53125 \n",
      " The score after the 64th episode is 9.523076923076923 \n",
      " The score after the 65th episode is 9.515151515151516 \n",
      " The score after the 66th episode is 9.507462686567164 \n",
      " The score after the 67th episode is 9.514705882352942 \n",
      " The score after the 68th episode is 9.55072463768116 \n",
      " The score after the 69th episode is 9.557142857142857 \n",
      " The score after the 70th episode is 9.56338028169014 \n",
      " The score after the 71th episode is 9.555555555555555 \n",
      " The score after the 72th episode is 9.547945205479452 \n",
      " The score after the 73th episode is 9.554054054054054 \n",
      " The score after the 74th episode is 9.56 \n",
      " The score after the 75th episode is 9.552631578947368 \n",
      " The score after the 76th episode is 9.545454545454545 \n",
      " The score after the 77th episode is 9.551282051282051 \n",
      " The score after the 78th episode is 9.556962025316455 \n",
      " The score after the 79th episode is 9.55 \n",
      " The score after the 80th episode is 9.530864197530864 \n",
      " The score after the 81th episode is 9.536585365853659 \n",
      " The score after the 82th episode is 9.53012048192771 \n",
      " The score after the 83th episode is 9.535714285714286 \n",
      " The score after the 84th episode is 9.529411764705882 \n",
      " The score after the 85th episode is 9.55813953488372 \n",
      " The score after the 86th episode is 9.563218390804598 \n",
      " The score after the 87th episode is 9.568181818181818 \n",
      " The score after the 88th episode is 9.573033707865168 \n",
      " The score after the 89th episode is 9.6 \n",
      " The score after the 90th episode is 9.626373626373626 \n",
      " The score after the 91th episode is 9.630434782608695 \n",
      " The score after the 92th episode is 9.634408602150538 \n",
      " The score after the 93th episode is 9.638297872340425 \n",
      " The score after the 94th episode is 9.631578947368421 \n",
      " The score after the 95th episode is 9.635416666666666 \n",
      " The score after the 96th episode is 9.628865979381443 \n",
      " The score after the 97th episode is 9.622448979591837 \n",
      " The score after the 98th episode is 9.616161616161616 \n",
      " The score after the 99th episode is 9.61 \n",
      " The score after the 100th episode is 9.61 \n",
      " The score after the 101th episode is 9.6 \n",
      " The score after the 102th episode is 9.59 \n",
      " The score after the 103th episode is 9.61 \n",
      " The score after the 104th episode is 9.59 \n",
      " The score after the 105th episode is 9.63 \n",
      " The score after the 106th episode is 9.63 \n",
      " The score after the 107th episode is 9.62 \n",
      " The score after the 108th episode is 9.71 \n",
      " The score after the 109th episode is 9.68 \n",
      " The score after the 110th episode is 9.71 \n",
      " The score after the 111th episode is 9.88 \n",
      " The score after the 112th episode is 9.93 \n",
      " The score after the 113th episode is 9.95 \n",
      " The score after the 114th episode is 9.99 \n",
      " The score after the 115th episode is 10.29 \n",
      " The score after the 116th episode is 10.44 \n",
      " The score after the 117th episode is 10.56 \n",
      " The score after the 118th episode is 11.39 \n",
      " The score after the 119th episode is 11.72 \n",
      " The score after the 120th episode is 12.42 \n",
      " The score after the 121th episode is 13.91 \n",
      " The score after the 122th episode is 15.3 \n",
      " The score after the 123th episode is 16.38 \n",
      " The score after the 124th episode is 17.88 \n",
      " The score after the 125th episode is 18.77 \n",
      " The score after the 126th episode is 20.06 \n",
      " The score after the 127th episode is 21.65 \n",
      " The score after the 128th episode is 22.03 \n",
      " The score after the 129th episode is 23.46 \n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement training logic for CartPole environment here\n",
    "# Remember to use the ExperienceBuffer and a target network\n",
    "# Details can be found in the book sent in the group\n",
    "class train_cartpole_model : \n",
    "    def __init__ (self, gamma, epsilon, lr, input_dims, batch_size, n_actions, eps_dec = 5e-4, eps_end = 1e-1, capacity = 1000) : \n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon \n",
    "        self.eps_dec = eps_dec \n",
    "        self.eps_end = eps_end\n",
    "        self.lr = lr \n",
    "        self.capacity = capacity\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.batch_size = batch_size \n",
    "        self.cnt = 0 \n",
    "\n",
    "        self.deepQ = DQN(input_dims,n_actions,self.lr)\n",
    "        self.buffer = ExperienceBuffer(self.capacity) \n",
    "\n",
    "    def take_action(self,observation) : \n",
    "        # Observation contains the state of a game in a batch for training \n",
    "        if np.random.uniform(0,1) < self.epsilon : \n",
    "            action = np.random.choice(self.action_space) \n",
    "        else : \n",
    "            observation = torch.tensor([observation])\n",
    "            action_space = self.deepQ.forward(observation)\n",
    "            action_space = action_space.detach().numpy()\n",
    "            action = np.argmax(action_space)\n",
    "        self.cnt += 1 \n",
    "        return action \n",
    "\n",
    "\n",
    "    def train_model(self) : \n",
    "        if self.cnt < self.capacity : \n",
    "            return \n",
    "        self.deepQ.optimizer.zero_grad() \n",
    "        # Select a batch and train on that \n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        states      = torch.tensor(states,      dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions     = torch.tensor(actions,     dtype=torch.int64)   \n",
    "        rewards     = torch.tensor(rewards,     dtype=torch.float32)\n",
    "        dones       = torch.tensor(dones,       dtype=torch.bool)\n",
    "    \n",
    "        q_eval_all = self.deepQ(states)   # Give the state-action matrix only for the relevant actions  \n",
    "        q_eval = q_eval_all.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            q_next_all = self.deepQ(next_states)           # shape: (batch, n_actions)\n",
    "            q_next_max = q_next_all.max(dim=1).values      # shape: (batch,)\n",
    "            q_next_max[dones] = 0                          # zero for terminal states\n",
    "\n",
    "        q_target = rewards + self.gamma * q_next_max\n",
    "        loss = self.deepQ.loss(q_target,q_eval) \n",
    "        loss.backward() \n",
    "        self.deepQ.optimizer.step() \n",
    "        self.epsilon = (self.epsilon*self.eps_dec if self.epsilon > self.eps_end else self.eps_end)\n",
    "\n",
    "# Train the model \n",
    "def train() : \n",
    "    env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "    n_actions = env.action_space.n \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    model = train_cartpole_model(0.99,0.05,0.003,obs_dim,128,n_actions) \n",
    "    rewards = [] \n",
    "    episodes = 130\n",
    "\n",
    "    for i in range(episodes) : \n",
    "        score = 0 \n",
    "        done = False \n",
    "        observation = env.reset()[0]\n",
    "        while not done : \n",
    "            action = model.take_action(observation) \n",
    "            next_state, reward, done, _ , _  = env.step(action)\n",
    "            score += reward \n",
    "            model.buffer.push(observation,action,reward,next_state,done) \n",
    "            model.train_model() \n",
    "            observation = next_state\n",
    " \n",
    "        rewards.append(score)\n",
    "\n",
    "        print(f\" The score after the {i}th episode is {np.mean(rewards[-100:])} \")\n",
    "    return model.deepQ \n",
    "\n",
    "model = train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6493553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cartpole_model(model, episodes=10, render=True):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards) / episodes\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8643afe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 106.0\n",
      "Episode 2: Reward = 109.0\n",
      "Episode 3: Reward = 115.0\n",
      "Episode 4: Reward = 109.0\n",
      "Episode 5: Reward = 111.0\n",
      "Episode 6: Reward = 114.0\n",
      "Episode 7: Reward = 112.0\n",
      "Episode 8: Reward = 115.0\n",
      "Episode 9: Reward = 114.0\n",
      "Episode 10: Reward = 112.0\n",
      "Average reward over 10 episodes: 111.7\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run evaluation for cartpole here\n",
    "evaluate_cartpole_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c4213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeGame(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
    "\n",
    "    def __init__(self, size=10, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.cell_size = 30\n",
    "        self.screen_size = self.size * self.cell_size\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(4)  # 0: right, 1: up, 2: left, 3: down\n",
    "        self.observation_space = gym.spaces.Box(0, 2, shape=(self.size, self.size), dtype=np.uint8)\n",
    "\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "\n",
    "        self.snake = deque()\n",
    "        self.food = None\n",
    "        self.direction = [1, 0]\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.snake.clear()\n",
    "        mid = self.size // 2\n",
    "        self.snake.appendleft([mid, mid])\n",
    "        self.direction = [1, 0]\n",
    "        self._place_food()\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_init()\n",
    "\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # 1) detect invalid 180° reverse attempts\n",
    "        if (action == 0 and self.direction == [-1, 0]) or \\\n",
    "           (action == 2 and self.direction == [ 1, 0]) or \\\n",
    "           (action == 1 and self.direction == [ 0, 1]) or \\\n",
    "           (action == 3 and self.direction == [ 0,-1]):\n",
    "            reward = -0.5\n",
    "        else:\n",
    "            reward = -0.01\n",
    "            # apply valid direction change:\n",
    "            if action == 0: self.direction = [1, 0]\n",
    "            elif action == 1: self.direction = [0,-1]\n",
    "            elif action == 2: self.direction = [-1,0]\n",
    "            elif action == 3: self.direction = [0, 1]\n",
    "        \n",
    "        head = self.snake[0]\n",
    "        new_head = [head[0] + self.direction[0], head[1] + self.direction[1]]\n",
    "        \n",
    "        terminated = False\n",
    "        \n",
    "        # 2) wall‐collision\n",
    "        if not (0 <= new_head[0] < self.size and 0 <= new_head[1] < self.size):\n",
    "            terminated = True\n",
    "            reward = -1.0\n",
    "        else:\n",
    "            # 3) self‐collision\n",
    "            body = list(self.snake)[:-1] if new_head != self.food else list(self.snake)\n",
    "            if new_head in body:\n",
    "                terminated = True\n",
    "                reward = -1.0\n",
    "        \n",
    "        # 4) advance snake (unless dead)\n",
    "        if not terminated:\n",
    "            self.snake.appendleft(new_head)\n",
    "            if new_head == self.food:\n",
    "                reward = +1.0                # eating food\n",
    "                self._place_food()\n",
    "            else:\n",
    "                self.snake.pop()\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        # truncated is always False for now\n",
    "        return obs, reward, terminated, False, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        board = np.zeros((self.size, self.size), dtype=np.uint8)\n",
    "        # mark snake\n",
    "        for x, y in self.snake:\n",
    "            board[y, x] = 1\n",
    "        # mark food\n",
    "        if self.food is not None:\n",
    "            fx, fy = self.food\n",
    "            board[fy, fx] = 2\n",
    "        return board\n",
    "\n",
    "    def _place_food(self):\n",
    "        positions = set(tuple(p) for p in self.snake)\n",
    "        empty = [(x, y) for x in range(self.size) for y in range(self.size) if (x, y) not in positions]\n",
    "        self.food = list(random.choice(empty)) if empty else None\n",
    "\n",
    "    def render(self):\n",
    "        if self.screen is None:\n",
    "            self._render_init()\n",
    "\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        for x, y in self.snake:\n",
    "            pygame.draw.rect(\n",
    "                self.screen, (0, 255, 0),\n",
    "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "        if self.food:\n",
    "            fx, fy = self.food\n",
    "            pygame.draw.rect(\n",
    "                self.screen, (255, 0, 0),\n",
    "                pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def _render_init(self):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.size * self.cell_size, self.size * self.cell_size))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen:\n",
    "            pygame.quit()\n",
    "            self.screen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d3ffb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward=-1.18, eps=1.000\n",
      "Episode 2: Reward=-1.56, eps=1.000\n",
      "Episode 3: Reward=-2.61, eps=1.000\n",
      "Episode 4: Reward=-2.09, eps=1.000\n",
      "Episode 5: Reward=-1.58, eps=1.000\n",
      "Episode 6: Reward=-2.67, eps=0.998\n",
      "Episode 7: Reward=-2.09, eps=0.996\n",
      "Episode 8: Reward=-3.09, eps=0.995\n",
      "Episode 9: Reward=-3.61, eps=0.993\n",
      "Episode 10: Reward=-2.61, eps=0.991\n",
      "Episode 11: Reward=-3.64, eps=0.989\n",
      "Episode 12: Reward=-2.52, eps=0.989\n",
      "Episode 13: Reward=-1.58, eps=0.988\n",
      "Episode 14: Reward=-2.53, eps=0.987\n",
      "Episode 15: Reward=-2.06, eps=0.986\n",
      "Episode 16: Reward=-1.05, eps=0.985\n",
      "Episode 17: Reward=-2.06, eps=0.984\n",
      "Episode 18: Reward=-4.63, eps=0.982\n",
      "Episode 19: Reward=-1.14, eps=0.981\n",
      "Episode 20: Reward=-5.21, eps=0.977\n",
      "Episode 21: Reward=-1.54, eps=0.977\n",
      "Episode 22: Reward=-2.55, eps=0.976\n",
      "Episode 23: Reward=-2.57, eps=0.975\n",
      "Episode 24: Reward=-3.13, eps=0.973\n",
      "Episode 25: Reward=-2.64, eps=0.971\n",
      "Episode 26: Reward=-3.71, eps=0.968\n",
      "Episode 27: Reward=-3.61, eps=0.966\n",
      "Episode 28: Reward=-2.05, eps=0.966\n",
      "Episode 29: Reward=-1.55, eps=0.965\n",
      "Episode 30: Reward=-2.59, eps=0.963\n",
      "Episode 31: Reward=-1.06, eps=0.963\n",
      "Episode 32: Reward=-2.61, eps=0.961\n",
      "Episode 33: Reward=-2.08, eps=0.960\n",
      "Episode 34: Reward=-3.10, eps=0.958\n",
      "Episode 35: Reward=-3.70, eps=0.956\n",
      "Episode 36: Reward=-2.08, eps=0.954\n",
      "Episode 37: Reward=-5.26, eps=0.951\n",
      "Episode 38: Reward=-2.51, eps=0.950\n",
      "Episode 39: Reward=-5.26, eps=0.946\n",
      "Episode 40: Reward=-3.65, eps=0.944\n",
      "Episode 41: Reward=-2.06, eps=0.943\n",
      "Episode 42: Reward=-1.64, eps=0.941\n",
      "Episode 43: Reward=-3.19, eps=0.939\n",
      "Episode 44: Reward=-1.08, eps=0.938\n",
      "Episode 45: Reward=-2.02, eps=0.938\n",
      "Episode 46: Reward=-2.65, eps=0.936\n",
      "Episode 47: Reward=-1.05, eps=0.935\n",
      "Episode 48: Reward=-3.80, eps=0.932\n",
      "Episode 49: Reward=-7.84, eps=0.927\n",
      "Episode 50: Reward=-2.05, eps=0.926\n",
      "Episode 51: Reward=-4.05, eps=0.925\n",
      "Episode 52: Reward=-1.05, eps=0.924\n",
      "Episode 53: Reward=-3.11, eps=0.923\n",
      "Episode 54: Reward=-4.16, eps=0.920\n",
      "Episode 55: Reward=-5.70, eps=0.917\n",
      "Episode 56: Reward=-4.18, eps=0.915\n",
      "Episode 57: Reward=-1.54, eps=0.914\n",
      "Episode 58: Reward=-5.72, eps=0.911\n",
      "Episode 59: Reward=-2.06, eps=0.910\n",
      "Episode 60: Reward=-2.57, eps=0.909\n",
      "Episode 61: Reward=-1.56, eps=0.908\n",
      "Episode 62: Reward=-2.09, eps=0.907\n",
      "Episode 63: Reward=-3.11, eps=0.905\n",
      "Episode 64: Reward=-2.10, eps=0.904\n",
      "Episode 65: Reward=-7.33, eps=0.899\n",
      "Episode 66: Reward=-1.55, eps=0.899\n",
      "Episode 67: Reward=-1.57, eps=0.898\n",
      "Episode 68: Reward=-1.60, eps=0.897\n",
      "Episode 69: Reward=-2.58, eps=0.895\n",
      "Episode 70: Reward=-2.68, eps=0.893\n",
      "Episode 71: Reward=-3.09, eps=0.892\n",
      "Episode 72: Reward=-2.51, eps=0.891\n",
      "Episode 73: Reward=-2.64, eps=0.890\n",
      "Episode 74: Reward=-2.04, eps=0.889\n",
      "Episode 75: Reward=-3.22, eps=0.886\n",
      "Episode 76: Reward=-4.64, eps=0.884\n",
      "Episode 77: Reward=-2.05, eps=0.883\n",
      "Episode 78: Reward=-1.07, eps=0.882\n",
      "Episode 79: Reward=-3.70, eps=0.880\n",
      "Episode 80: Reward=-0.58, eps=0.879\n",
      "Episode 81: Reward=-1.69, eps=0.876\n",
      "Episode 82: Reward=-3.19, eps=0.873\n",
      "Episode 83: Reward=-2.62, eps=0.871\n",
      "Episode 84: Reward=-3.57, eps=0.870\n",
      "Episode 85: Reward=-1.54, eps=0.869\n",
      "Episode 86: Reward=-3.69, eps=0.867\n",
      "Episode 87: Reward=-1.56, eps=0.866\n",
      "Episode 88: Reward=-2.62, eps=0.865\n",
      "Episode 89: Reward=-2.04, eps=0.864\n",
      "Episode 90: Reward=-1.54, eps=0.863\n",
      "Episode 91: Reward=-2.60, eps=0.862\n",
      "Episode 92: Reward=-3.54, eps=0.861\n",
      "Episode 93: Reward=-2.02, eps=0.860\n",
      "Episode 94: Reward=-2.03, eps=0.860\n",
      "Episode 95: Reward=-1.56, eps=0.859\n",
      "Episode 96: Reward=-2.06, eps=0.858\n",
      "Episode 97: Reward=-3.18, eps=0.856\n",
      "Episode 98: Reward=-2.12, eps=0.854\n",
      "Episode 99: Reward=-2.09, eps=0.853\n",
      "Episode 100: Reward=-1.10, eps=0.852\n",
      "Episode 101: Reward=-2.06, eps=0.851\n",
      "Episode 102: Reward=-2.66, eps=0.849\n",
      "Episode 103: Reward=-1.65, eps=0.847\n",
      "Episode 104: Reward=-3.69, eps=0.845\n",
      "Episode 105: Reward=-2.06, eps=0.844\n",
      "Episode 106: Reward=-4.17, eps=0.842\n",
      "Episode 107: Reward=-3.15, eps=0.840\n",
      "Episode 108: Reward=-1.07, eps=0.839\n",
      "Episode 109: Reward=-0.05, eps=0.838\n",
      "Episode 110: Reward=-2.03, eps=0.837\n",
      "Episode 111: Reward=-1.54, eps=0.837\n",
      "Episode 112: Reward=-2.60, eps=0.835\n",
      "Episode 113: Reward=-2.73, eps=0.832\n",
      "Episode 114: Reward=-0.55, eps=0.832\n",
      "Episode 115: Reward=-2.05, eps=0.831\n",
      "Episode 116: Reward=-1.57, eps=0.830\n",
      "Episode 117: Reward=-3.20, eps=0.827\n",
      "Episode 118: Reward=-1.07, eps=0.826\n",
      "Episode 119: Reward=-1.06, eps=0.826\n",
      "Episode 120: Reward=-2.55, eps=0.825\n",
      "Episode 121: Reward=-2.05, eps=0.824\n",
      "Episode 122: Reward=-2.05, eps=0.823\n",
      "Episode 123: Reward=-2.55, eps=0.822\n",
      "Episode 124: Reward=-1.06, eps=0.821\n",
      "Episode 125: Reward=-2.53, eps=0.821\n",
      "Episode 126: Reward=-1.13, eps=0.819\n",
      "Episode 127: Reward=-0.06, eps=0.818\n",
      "Episode 128: Reward=-2.10, eps=0.816\n",
      "Episode 129: Reward=-4.76, eps=0.813\n",
      "Episode 130: Reward=-2.15, eps=0.811\n",
      "Episode 131: Reward=-3.08, eps=0.810\n",
      "Episode 132: Reward=-4.74, eps=0.806\n",
      "Episode 133: Reward=-1.60, eps=0.805\n",
      "Episode 134: Reward=-2.09, eps=0.804\n",
      "Episode 135: Reward=-1.56, eps=0.803\n",
      "Episode 136: Reward=-1.57, eps=0.802\n",
      "Episode 137: Reward=-1.54, eps=0.801\n",
      "Episode 138: Reward=-2.06, eps=0.801\n",
      "Episode 139: Reward=-1.62, eps=0.799\n",
      "Episode 140: Reward=-1.61, eps=0.798\n",
      "Episode 141: Reward=-4.09, eps=0.796\n",
      "Episode 142: Reward=-3.07, eps=0.795\n",
      "Episode 143: Reward=-1.11, eps=0.794\n",
      "Episode 144: Reward=-2.03, eps=0.793\n",
      "Episode 145: Reward=-0.69, eps=0.791\n",
      "Episode 146: Reward=-2.53, eps=0.790\n",
      "Episode 147: Reward=-2.04, eps=0.789\n",
      "Episode 148: Reward=-1.66, eps=0.788\n",
      "Episode 149: Reward=-3.13, eps=0.786\n",
      "Episode 150: Reward=-1.16, eps=0.783\n",
      "Episode 151: Reward=-1.58, eps=0.782\n",
      "Episode 152: Reward=-3.68, eps=0.780\n",
      "Episode 153: Reward=-1.08, eps=0.779\n",
      "Episode 154: Reward=-2.04, eps=0.778\n",
      "Episode 155: Reward=-1.07, eps=0.778\n",
      "Episode 156: Reward=-5.18, eps=0.775\n",
      "Episode 157: Reward=-2.58, eps=0.774\n",
      "Episode 158: Reward=-1.05, eps=0.773\n",
      "Episode 159: Reward=-2.66, eps=0.771\n",
      "Episode 160: Reward=-5.75, eps=0.768\n",
      "Episode 161: Reward=-2.07, eps=0.766\n",
      "Episode 162: Reward=-3.12, eps=0.765\n",
      "Episode 163: Reward=-1.06, eps=0.764\n",
      "Episode 164: Reward=-2.05, eps=0.763\n",
      "Episode 165: Reward=-2.54, eps=0.762\n",
      "Episode 166: Reward=-2.10, eps=0.761\n",
      "Episode 167: Reward=-2.59, eps=0.760\n",
      "Episode 168: Reward=-2.04, eps=0.759\n",
      "Episode 169: Reward=-1.58, eps=0.758\n",
      "Episode 170: Reward=-2.10, eps=0.756\n",
      "Episode 171: Reward=-0.06, eps=0.756\n",
      "Episode 172: Reward=-2.59, eps=0.754\n",
      "Episode 173: Reward=-2.59, eps=0.753\n",
      "Episode 174: Reward=-1.56, eps=0.752\n",
      "Episode 175: Reward=-3.64, eps=0.750\n",
      "Episode 176: Reward=-0.56, eps=0.749\n",
      "Episode 177: Reward=-2.06, eps=0.748\n",
      "Episode 178: Reward=-2.54, eps=0.748\n",
      "Episode 179: Reward=-2.09, eps=0.746\n",
      "Episode 180: Reward=-1.58, eps=0.745\n",
      "Episode 181: Reward=-1.59, eps=0.744\n",
      "Episode 182: Reward=-1.06, eps=0.743\n",
      "Episode 183: Reward=-4.20, eps=0.741\n",
      "Episode 184: Reward=-1.58, eps=0.740\n",
      "Episode 185: Reward=-3.14, eps=0.738\n",
      "Episode 186: Reward=-2.56, eps=0.737\n",
      "Episode 187: Reward=-1.62, eps=0.735\n",
      "Episode 188: Reward=-2.03, eps=0.735\n",
      "Episode 189: Reward=-1.65, eps=0.733\n",
      "Episode 190: Reward=-3.19, eps=0.730\n",
      "Episode 191: Reward=-1.53, eps=0.730\n",
      "Episode 192: Reward=-3.53, eps=0.729\n",
      "Episode 193: Reward=-2.06, eps=0.728\n",
      "Episode 194: Reward=-3.66, eps=0.726\n",
      "Episode 195: Reward=-5.13, eps=0.724\n",
      "Episode 196: Reward=-0.64, eps=0.721\n",
      "Episode 197: Reward=-4.70, eps=0.719\n",
      "Episode 198: Reward=-1.55, eps=0.718\n",
      "Episode 199: Reward=-6.25, eps=0.714\n",
      "Episode 200: Reward=-1.08, eps=0.713\n",
      "Episode 201: Reward=-1.63, eps=0.712\n",
      "Episode 202: Reward=-3.54, eps=0.711\n",
      "Episode 203: Reward=-2.53, eps=0.710\n",
      "Episode 204: Reward=-6.75, eps=0.706\n",
      "Episode 205: Reward=-3.23, eps=0.703\n",
      "Episode 206: Reward=-2.53, eps=0.702\n",
      "Episode 207: Reward=-2.07, eps=0.701\n",
      "Episode 208: Reward=-3.09, eps=0.700\n",
      "Episode 209: Reward=-2.08, eps=0.699\n",
      "Episode 210: Reward=-6.22, eps=0.695\n",
      "Episode 211: Reward=-1.04, eps=0.695\n",
      "Episode 212: Reward=-2.56, eps=0.694\n",
      "Episode 213: Reward=-1.64, eps=0.692\n",
      "Episode 214: Reward=-2.06, eps=0.691\n",
      "Episode 215: Reward=-1.61, eps=0.690\n",
      "Episode 216: Reward=-2.16, eps=0.688\n",
      "Episode 217: Reward=-2.02, eps=0.688\n",
      "Episode 218: Reward=-2.05, eps=0.687\n",
      "Episode 219: Reward=-2.09, eps=0.686\n",
      "Episode 220: Reward=-2.13, eps=0.684\n",
      "Episode 221: Reward=-2.09, eps=0.683\n",
      "Episode 222: Reward=-2.07, eps=0.682\n",
      "Episode 223: Reward=-2.17, eps=0.679\n",
      "Episode 224: Reward=-1.59, eps=0.678\n",
      "Episode 225: Reward=-0.12, eps=0.677\n",
      "Episode 226: Reward=-4.17, eps=0.674\n",
      "Episode 227: Reward=-2.03, eps=0.674\n",
      "Episode 228: Reward=-3.58, eps=0.672\n",
      "Episode 229: Reward=-1.06, eps=0.672\n",
      "Episode 230: Reward=-1.08, eps=0.670\n",
      "Episode 231: Reward=-1.11, eps=0.669\n",
      "Episode 232: Reward=-3.73, eps=0.666\n",
      "Episode 233: Reward=-5.21, eps=0.663\n",
      "Episode 234: Reward=-3.57, eps=0.661\n",
      "Episode 235: Reward=-0.04, eps=0.661\n",
      "Episode 236: Reward=-2.04, eps=0.660\n",
      "Episode 237: Reward=-2.09, eps=0.659\n",
      "Episode 238: Reward=-0.62, eps=0.657\n",
      "Episode 239: Reward=-2.64, eps=0.655\n",
      "Episode 240: Reward=-1.55, eps=0.655\n",
      "Episode 241: Reward=-2.10, eps=0.653\n",
      "Episode 242: Reward=-2.07, eps=0.652\n",
      "Episode 243: Reward=-4.29, eps=0.649\n",
      "Episode 244: Reward=-0.62, eps=0.647\n",
      "Episode 245: Reward=-1.07, eps=0.646\n",
      "Episode 246: Reward=-1.05, eps=0.646\n",
      "Episode 247: Reward=-1.54, eps=0.645\n",
      "Episode 248: Reward=-1.61, eps=0.644\n",
      "Episode 249: Reward=-2.57, eps=0.642\n",
      "Episode 250: Reward=-3.72, eps=0.640\n",
      "Episode 251: Reward=-2.04, eps=0.639\n",
      "Episode 252: Reward=-3.71, eps=0.636\n",
      "Episode 253: Reward=-1.53, eps=0.636\n",
      "Episode 254: Reward=-3.17, eps=0.634\n",
      "Episode 255: Reward=-3.04, eps=0.633\n",
      "Episode 256: Reward=-1.53, eps=0.632\n",
      "Episode 257: Reward=-2.59, eps=0.631\n",
      "Episode 258: Reward=-2.05, eps=0.630\n",
      "Episode 259: Reward=-0.13, eps=0.629\n",
      "Episode 260: Reward=-3.15, eps=0.627\n",
      "Episode 261: Reward=-2.60, eps=0.625\n",
      "Episode 262: Reward=-1.09, eps=0.624\n",
      "Episode 263: Reward=-1.05, eps=0.624\n",
      "Episode 264: Reward=-1.61, eps=0.622\n",
      "Episode 265: Reward=-1.11, eps=0.621\n",
      "Episode 266: Reward=-1.53, eps=0.621\n",
      "Episode 267: Reward=-2.58, eps=0.619\n",
      "Episode 268: Reward=-0.63, eps=0.618\n",
      "Episode 269: Reward=-3.53, eps=0.617\n",
      "Episode 270: Reward=-6.68, eps=0.614\n",
      "Episode 271: Reward=-2.54, eps=0.613\n",
      "Episode 272: Reward=-1.65, eps=0.611\n",
      "Episode 273: Reward=-1.05, eps=0.610\n",
      "Episode 274: Reward=-2.05, eps=0.610\n",
      "Episode 275: Reward=-3.69, eps=0.607\n",
      "Episode 276: Reward=-2.07, eps=0.606\n",
      "Episode 277: Reward=-2.71, eps=0.604\n",
      "Episode 278: Reward=-1.59, eps=0.603\n",
      "Episode 279: Reward=-1.66, eps=0.600\n",
      "Episode 280: Reward=-4.07, eps=0.599\n",
      "Episode 281: Reward=-3.07, eps=0.598\n",
      "Episode 282: Reward=-1.06, eps=0.597\n",
      "Episode 283: Reward=-1.54, eps=0.597\n",
      "Episode 284: Reward=-5.78, eps=0.593\n",
      "Episode 285: Reward=-1.61, eps=0.591\n",
      "Episode 286: Reward=-2.22, eps=0.589\n",
      "Episode 287: Reward=-2.05, eps=0.588\n",
      "Episode 288: Reward=-2.07, eps=0.587\n",
      "Episode 289: Reward=-1.56, eps=0.586\n",
      "Episode 290: Reward=-1.65, eps=0.585\n",
      "Episode 291: Reward=-3.12, eps=0.583\n",
      "Episode 292: Reward=-1.61, eps=0.581\n",
      "Episode 293: Reward=-2.19, eps=0.579\n",
      "Episode 294: Reward=-3.11, eps=0.578\n",
      "Episode 295: Reward=-4.16, eps=0.575\n",
      "Episode 296: Reward=-2.05, eps=0.574\n",
      "Episode 297: Reward=-2.59, eps=0.573\n",
      "Episode 298: Reward=-3.08, eps=0.572\n",
      "Episode 299: Reward=-1.05, eps=0.571\n",
      "Episode 300: Reward=-1.60, eps=0.570\n",
      "Episode 301: Reward=-2.07, eps=0.569\n",
      "Episode 302: Reward=-2.63, eps=0.567\n",
      "Episode 303: Reward=-2.03, eps=0.567\n",
      "Episode 304: Reward=-1.60, eps=0.566\n",
      "Episode 305: Reward=-3.28, eps=0.562\n",
      "Episode 306: Reward=-3.13, eps=0.560\n",
      "Episode 307: Reward=-0.64, eps=0.558\n",
      "Episode 308: Reward=-1.07, eps=0.557\n",
      "Episode 309: Reward=-2.63, eps=0.555\n",
      "Episode 310: Reward=-1.64, eps=0.554\n",
      "Episode 311: Reward=-5.20, eps=0.551\n",
      "Episode 312: Reward=-1.04, eps=0.550\n",
      "Episode 313: Reward=-1.16, eps=0.549\n",
      "Episode 314: Reward=-2.64, eps=0.547\n",
      "Episode 315: Reward=-1.09, eps=0.546\n",
      "Episode 316: Reward=-1.05, eps=0.545\n",
      "Episode 317: Reward=-4.11, eps=0.543\n",
      "Episode 318: Reward=-2.61, eps=0.542\n",
      "Episode 319: Reward=-2.71, eps=0.539\n",
      "Episode 320: Reward=-1.53, eps=0.538\n",
      "Episode 321: Reward=-2.15, eps=0.537\n",
      "Episode 322: Reward=-3.06, eps=0.535\n",
      "Episode 323: Reward=-1.57, eps=0.535\n",
      "Episode 324: Reward=-2.06, eps=0.534\n",
      "Episode 325: Reward=-2.05, eps=0.533\n",
      "Episode 326: Reward=-9.27, eps=0.528\n",
      "Episode 327: Reward=-2.65, eps=0.527\n",
      "Episode 328: Reward=-2.70, eps=0.524\n",
      "Episode 329: Reward=-1.09, eps=0.523\n",
      "Episode 330: Reward=-2.61, eps=0.521\n",
      "Episode 331: Reward=-3.71, eps=0.519\n",
      "Episode 332: Reward=-2.07, eps=0.518\n",
      "Episode 333: Reward=-6.26, eps=0.514\n",
      "Episode 334: Reward=-4.23, eps=0.511\n",
      "Episode 335: Reward=-2.60, eps=0.510\n",
      "Episode 336: Reward=-2.16, eps=0.507\n",
      "Episode 337: Reward=-3.07, eps=0.506\n",
      "Episode 338: Reward=-2.10, eps=0.505\n",
      "Episode 339: Reward=-1.07, eps=0.503\n",
      "Episode 340: Reward=-3.12, eps=0.502\n",
      "Episode 341: Reward=-2.58, eps=0.501\n",
      "Episode 342: Reward=-2.04, eps=0.500\n",
      "Episode 343: Reward=-4.65, eps=0.498\n",
      "Episode 344: Reward=-1.58, eps=0.497\n",
      "Episode 345: Reward=-2.56, eps=0.496\n",
      "Episode 346: Reward=-2.52, eps=0.495\n",
      "Episode 347: Reward=-1.08, eps=0.494\n",
      "Episode 348: Reward=-1.54, eps=0.493\n",
      "Episode 349: Reward=-3.21, eps=0.491\n",
      "Episode 350: Reward=-2.05, eps=0.490\n",
      "Episode 351: Reward=-2.53, eps=0.489\n",
      "Episode 352: Reward=-1.57, eps=0.488\n",
      "Episode 353: Reward=-3.55, eps=0.487\n",
      "Episode 354: Reward=-2.05, eps=0.486\n",
      "Episode 355: Reward=-3.08, eps=0.485\n",
      "Episode 356: Reward=-2.03, eps=0.484\n",
      "Episode 357: Reward=-2.08, eps=0.483\n",
      "Episode 358: Reward=-1.57, eps=0.482\n",
      "Episode 359: Reward=-1.60, eps=0.481\n",
      "Episode 360: Reward=-1.07, eps=0.480\n",
      "Episode 361: Reward=-1.55, eps=0.480\n",
      "Episode 362: Reward=-5.09, eps=0.478\n",
      "Episode 363: Reward=-6.18, eps=0.475\n",
      "Episode 364: Reward=-3.00, eps=0.474\n",
      "Episode 365: Reward=-1.58, eps=0.473\n",
      "Episode 366: Reward=-2.15, eps=0.471\n",
      "Episode 367: Reward=-1.68, eps=0.469\n",
      "Episode 368: Reward=-3.72, eps=0.466\n",
      "Episode 369: Reward=-1.57, eps=0.465\n",
      "Episode 370: Reward=-2.51, eps=0.464\n",
      "Episode 371: Reward=-3.21, eps=0.461\n",
      "Episode 372: Reward=-0.58, eps=0.460\n",
      "Episode 373: Reward=-1.58, eps=0.459\n",
      "Episode 374: Reward=-1.58, eps=0.458\n",
      "Episode 375: Reward=-1.59, eps=0.457\n",
      "Episode 376: Reward=-1.53, eps=0.457\n",
      "Episode 377: Reward=-1.70, eps=0.454\n",
      "Episode 378: Reward=-2.05, eps=0.453\n",
      "Episode 379: Reward=-1.04, eps=0.453\n",
      "Episode 380: Reward=-2.14, eps=0.451\n",
      "Episode 381: Reward=-1.14, eps=0.450\n",
      "Episode 382: Reward=-1.10, eps=0.449\n",
      "Episode 383: Reward=-3.13, eps=0.447\n",
      "Episode 384: Reward=-1.11, eps=0.446\n",
      "Episode 385: Reward=-1.08, eps=0.445\n",
      "Episode 386: Reward=-2.06, eps=0.444\n",
      "Episode 387: Reward=-0.59, eps=0.443\n",
      "Episode 388: Reward=-2.56, eps=0.442\n",
      "Episode 389: Reward=-2.11, eps=0.440\n",
      "Episode 390: Reward=-2.54, eps=0.439\n",
      "Episode 391: Reward=-1.06, eps=0.439\n",
      "Episode 392: Reward=-1.06, eps=0.438\n",
      "Episode 393: Reward=-0.55, eps=0.437\n",
      "Episode 394: Reward=-1.59, eps=0.436\n",
      "Episode 395: Reward=-4.16, eps=0.434\n",
      "Episode 396: Reward=-1.12, eps=0.432\n",
      "Episode 397: Reward=-3.12, eps=0.431\n",
      "Episode 398: Reward=-1.06, eps=0.430\n",
      "Episode 399: Reward=-2.05, eps=0.429\n",
      "Episode 400: Reward=-2.56, eps=0.428\n",
      "Episode 401: Reward=-2.12, eps=0.427\n",
      "Episode 402: Reward=-2.58, eps=0.426\n",
      "Episode 403: Reward=-1.10, eps=0.424\n",
      "Episode 404: Reward=-1.60, eps=0.423\n",
      "Episode 405: Reward=-3.15, eps=0.421\n",
      "Episode 406: Reward=-2.04, eps=0.420\n",
      "Episode 407: Reward=-6.76, eps=0.416\n",
      "Episode 408: Reward=-2.04, eps=0.416\n",
      "Episode 409: Reward=-1.60, eps=0.414\n",
      "Episode 410: Reward=-2.11, eps=0.413\n",
      "Episode 411: Reward=-0.05, eps=0.412\n",
      "Episode 412: Reward=-1.55, eps=0.411\n",
      "Episode 413: Reward=-2.06, eps=0.410\n",
      "Episode 414: Reward=-2.08, eps=0.409\n",
      "Episode 415: Reward=-1.07, eps=0.408\n",
      "Episode 416: Reward=-4.68, eps=0.405\n",
      "Episode 417: Reward=-0.59, eps=0.404\n",
      "Episode 418: Reward=-2.11, eps=0.403\n",
      "Episode 419: Reward=-3.62, eps=0.401\n",
      "Episode 420: Reward=-3.15, eps=0.399\n",
      "Episode 421: Reward=-1.62, eps=0.397\n",
      "Episode 422: Reward=-1.06, eps=0.396\n",
      "Episode 423: Reward=-5.21, eps=0.393\n",
      "Episode 424: Reward=-4.19, eps=0.390\n",
      "Episode 425: Reward=-3.57, eps=0.389\n",
      "Episode 426: Reward=-0.59, eps=0.388\n",
      "Episode 427: Reward=-2.06, eps=0.387\n",
      "Episode 428: Reward=-2.60, eps=0.385\n",
      "Episode 429: Reward=-5.28, eps=0.382\n",
      "Episode 430: Reward=-2.04, eps=0.381\n",
      "Episode 431: Reward=-1.65, eps=0.379\n",
      "Episode 432: Reward=-3.66, eps=0.377\n",
      "Episode 433: Reward=-4.31, eps=0.373\n",
      "Episode 434: Reward=-3.17, eps=0.371\n",
      "Episode 435: Reward=-4.64, eps=0.368\n",
      "Episode 436: Reward=-1.59, eps=0.367\n",
      "Episode 437: Reward=-1.11, eps=0.366\n",
      "Episode 438: Reward=-1.65, eps=0.364\n",
      "Episode 439: Reward=-2.03, eps=0.363\n",
      "Episode 440: Reward=-1.11, eps=0.362\n",
      "Episode 441: Reward=-1.58, eps=0.361\n",
      "Episode 442: Reward=-1.55, eps=0.360\n",
      "Episode 443: Reward=-1.61, eps=0.359\n",
      "Episode 444: Reward=-3.27, eps=0.356\n",
      "Episode 445: Reward=-1.06, eps=0.355\n",
      "Episode 446: Reward=-1.54, eps=0.355\n",
      "Episode 447: Reward=-3.13, eps=0.353\n",
      "Episode 448: Reward=-2.11, eps=0.351\n",
      "Episode 449: Reward=-3.66, eps=0.349\n",
      "Episode 450: Reward=-1.11, eps=0.348\n",
      "Episode 451: Reward=-0.58, eps=0.347\n",
      "Episode 452: Reward=-1.56, eps=0.346\n",
      "Episode 453: Reward=-3.13, eps=0.344\n",
      "Episode 454: Reward=-2.68, eps=0.342\n",
      "Episode 455: Reward=-2.05, eps=0.341\n",
      "Episode 456: Reward=-2.26, eps=0.337\n",
      "Episode 457: Reward=-4.16, eps=0.335\n",
      "Episode 458: Reward=-0.60, eps=0.334\n",
      "Episode 459: Reward=-2.62, eps=0.332\n",
      "Episode 460: Reward=-1.54, eps=0.331\n",
      "Episode 461: Reward=-5.85, eps=0.327\n",
      "Episode 462: Reward=-2.04, eps=0.326\n",
      "Episode 463: Reward=-2.13, eps=0.324\n",
      "Episode 464: Reward=-2.13, eps=0.322\n",
      "Episode 465: Reward=-0.64, eps=0.321\n",
      "Episode 466: Reward=-1.59, eps=0.319\n",
      "Episode 467: Reward=-1.57, eps=0.318\n",
      "Episode 468: Reward=-2.11, eps=0.317\n",
      "Episode 469: Reward=-4.82, eps=0.313\n",
      "Episode 470: Reward=-1.08, eps=0.312\n",
      "Episode 471: Reward=-1.05, eps=0.311\n",
      "Episode 472: Reward=-2.65, eps=0.309\n",
      "Episode 473: Reward=-4.74, eps=0.306\n",
      "Episode 474: Reward=-2.54, eps=0.305\n",
      "Episode 475: Reward=-2.11, eps=0.304\n",
      "Episode 476: Reward=-2.13, eps=0.302\n",
      "Episode 477: Reward=-2.05, eps=0.301\n",
      "Episode 478: Reward=-2.73, eps=0.298\n",
      "Episode 479: Reward=-1.07, eps=0.297\n",
      "Episode 480: Reward=-5.34, eps=0.293\n",
      "Episode 481: Reward=-2.57, eps=0.292\n",
      "Episode 482: Reward=-2.57, eps=0.291\n",
      "Episode 483: Reward=-1.11, eps=0.289\n",
      "Episode 484: Reward=-0.57, eps=0.288\n",
      "Episode 485: Reward=-1.57, eps=0.288\n",
      "Episode 486: Reward=-1.02, eps=0.287\n",
      "Episode 487: Reward=-2.02, eps=0.286\n",
      "Episode 488: Reward=-2.02, eps=0.286\n",
      "Episode 489: Reward=-1.55, eps=0.285\n",
      "Episode 490: Reward=-3.78, eps=0.282\n",
      "Episode 491: Reward=-2.64, eps=0.280\n",
      "Episode 492: Reward=-2.14, eps=0.278\n",
      "Episode 493: Reward=-1.59, eps=0.277\n",
      "Episode 494: Reward=-1.58, eps=0.276\n",
      "Episode 495: Reward=-0.10, eps=0.275\n",
      "Episode 496: Reward=-2.08, eps=0.274\n",
      "Episode 497: Reward=-1.60, eps=0.273\n",
      "Episode 498: Reward=-3.12, eps=0.271\n",
      "Episode 499: Reward=-2.64, eps=0.269\n",
      "Episode 500: Reward=-1.11, eps=0.268\n",
      "Episode 501: Reward=-3.15, eps=0.266\n",
      "Episode 502: Reward=-0.57, eps=0.265\n",
      "Episode 503: Reward=-3.22, eps=0.262\n",
      "Episode 504: Reward=-1.12, eps=0.261\n",
      "Episode 505: Reward=-2.67, eps=0.259\n",
      "Episode 506: Reward=-2.06, eps=0.258\n",
      "Episode 507: Reward=-0.07, eps=0.257\n",
      "Episode 508: Reward=-2.24, eps=0.254\n",
      "Episode 509: Reward=-2.57, eps=0.253\n",
      "Episode 510: Reward=-2.08, eps=0.252\n",
      "Episode 511: Reward=-1.04, eps=0.251\n",
      "Episode 512: Reward=-0.06, eps=0.250\n",
      "Episode 513: Reward=-1.61, eps=0.249\n",
      "Episode 514: Reward=-6.12, eps=0.247\n",
      "Episode 515: Reward=-2.79, eps=0.243\n",
      "Episode 516: Reward=-2.09, eps=0.242\n",
      "Episode 517: Reward=-8.39, eps=0.237\n",
      "Episode 518: Reward=-3.80, eps=0.233\n",
      "Episode 519: Reward=-1.57, eps=0.232\n",
      "Episode 520: Reward=-3.69, eps=0.229\n",
      "Episode 521: Reward=-2.11, eps=0.228\n",
      "Episode 522: Reward=-1.61, eps=0.226\n",
      "Episode 523: Reward=-6.91, eps=0.221\n",
      "Episode 524: Reward=-3.16, eps=0.219\n",
      "Episode 525: Reward=-3.63, eps=0.217\n",
      "Episode 526: Reward=-1.61, eps=0.216\n",
      "Episode 527: Reward=-5.22, eps=0.212\n",
      "Episode 528: Reward=-5.89, eps=0.207\n",
      "Episode 529: Reward=-2.68, eps=0.205\n",
      "Episode 530: Reward=-1.69, eps=0.203\n",
      "Episode 531: Reward=-2.21, eps=0.200\n",
      "Episode 532: Reward=-2.10, eps=0.199\n",
      "Episode 533: Reward=-2.64, eps=0.197\n",
      "Episode 534: Reward=-4.22, eps=0.194\n",
      "Episode 535: Reward=-3.16, eps=0.192\n",
      "Episode 536: Reward=-2.60, eps=0.190\n",
      "Episode 537: Reward=-1.09, eps=0.189\n",
      "Episode 538: Reward=-2.22, eps=0.187\n",
      "Episode 539: Reward=-0.58, eps=0.186\n",
      "Episode 540: Reward=0.92, eps=0.184\n",
      "Episode 541: Reward=-1.08, eps=0.183\n",
      "Episode 542: Reward=-2.53, eps=0.183\n",
      "Episode 543: Reward=-10.98, eps=0.176\n",
      "Episode 544: Reward=-0.05, eps=0.175\n",
      "Episode 545: Reward=-1.04, eps=0.175\n",
      "Episode 546: Reward=-1.55, eps=0.174\n",
      "Episode 547: Reward=-1.61, eps=0.173\n",
      "Episode 548: Reward=-2.08, eps=0.172\n",
      "Episode 549: Reward=-2.62, eps=0.170\n",
      "Episode 550: Reward=-2.30, eps=0.166\n",
      "Episode 551: Reward=-1.54, eps=0.166\n",
      "Episode 552: Reward=-3.62, eps=0.164\n",
      "Episode 553: Reward=-2.12, eps=0.162\n",
      "Episode 554: Reward=-3.10, eps=0.161\n",
      "Episode 555: Reward=-1.69, eps=0.159\n",
      "Episode 556: Reward=-1.54, eps=0.158\n",
      "Episode 557: Reward=-3.14, eps=0.156\n",
      "Episode 558: Reward=-1.59, eps=0.155\n",
      "Episode 559: Reward=-1.14, eps=0.153\n",
      "Episode 560: Reward=-1.57, eps=0.152\n",
      "Episode 561: Reward=-2.25, eps=0.149\n",
      "Episode 562: Reward=-2.08, eps=0.148\n",
      "Episode 563: Reward=-2.11, eps=0.147\n",
      "Episode 564: Reward=-0.61, eps=0.145\n",
      "Episode 565: Reward=-4.63, eps=0.143\n",
      "Episode 566: Reward=-2.08, eps=0.142\n",
      "Episode 567: Reward=-1.64, eps=0.140\n",
      "Episode 568: Reward=-0.09, eps=0.139\n",
      "Episode 569: Reward=-1.08, eps=0.138\n",
      "Episode 570: Reward=-1.05, eps=0.138\n",
      "Episode 571: Reward=-1.66, eps=0.136\n",
      "Episode 572: Reward=-1.56, eps=0.135\n",
      "Episode 573: Reward=-3.78, eps=0.131\n",
      "Episode 574: Reward=-1.15, eps=0.130\n",
      "Episode 575: Reward=-1.56, eps=0.129\n",
      "Episode 576: Reward=-2.16, eps=0.127\n",
      "Episode 577: Reward=-0.58, eps=0.126\n",
      "Episode 578: Reward=-0.15, eps=0.124\n",
      "Episode 579: Reward=-7.33, eps=0.119\n",
      "Episode 580: Reward=-5.23, eps=0.116\n",
      "Episode 581: Reward=-6.10, eps=0.114\n",
      "Episode 582: Reward=-2.70, eps=0.111\n",
      "Episode 583: Reward=-1.07, eps=0.110\n",
      "Episode 584: Reward=-1.11, eps=0.109\n",
      "Episode 585: Reward=-2.67, eps=0.107\n",
      "Episode 586: Reward=-1.09, eps=0.106\n",
      "Episode 587: Reward=-1.13, eps=0.104\n",
      "Episode 588: Reward=-1.09, eps=0.103\n",
      "Episode 589: Reward=-1.07, eps=0.103\n",
      "Episode 590: Reward=-1.61, eps=0.101\n",
      "Episode 591: Reward=-0.03, eps=0.101\n",
      "Episode 592: Reward=-3.18, eps=0.100\n",
      "Episode 593: Reward=-3.12, eps=0.100\n",
      "Episode 594: Reward=-0.06, eps=0.100\n",
      "Episode 595: Reward=-3.26, eps=0.100\n",
      "Episode 596: Reward=-4.33, eps=0.100\n",
      "Episode 597: Reward=-1.61, eps=0.100\n",
      "Episode 598: Reward=-1.05, eps=0.100\n",
      "Episode 599: Reward=-1.08, eps=0.100\n",
      "Episode 600: Reward=-3.86, eps=0.100\n",
      "Episode 601: Reward=-1.09, eps=0.100\n",
      "Episode 602: Reward=-12.05, eps=0.100\n",
      "Episode 603: Reward=-4.35, eps=0.100\n",
      "Episode 604: Reward=-2.08, eps=0.100\n",
      "Episode 605: Reward=-1.54, eps=0.100\n",
      "Episode 606: Reward=-0.09, eps=0.100\n",
      "Episode 607: Reward=-2.65, eps=0.100\n",
      "Episode 608: Reward=-1.56, eps=0.100\n",
      "Episode 609: Reward=-1.09, eps=0.100\n",
      "Episode 610: Reward=-2.53, eps=0.100\n",
      "Episode 611: Reward=-0.05, eps=0.100\n",
      "Episode 612: Reward=-2.07, eps=0.100\n",
      "Episode 613: Reward=-1.59, eps=0.100\n",
      "Episode 614: Reward=-4.25, eps=0.100\n",
      "Episode 615: Reward=-1.59, eps=0.100\n",
      "Episode 616: Reward=-4.64, eps=0.100\n",
      "Episode 617: Reward=-1.09, eps=0.100\n",
      "Episode 618: Reward=-6.92, eps=0.100\n",
      "Episode 619: Reward=-5.85, eps=0.100\n",
      "Episode 620: Reward=-2.06, eps=0.100\n",
      "Episode 621: Reward=-2.06, eps=0.100\n",
      "Episode 622: Reward=-1.08, eps=0.100\n",
      "Episode 623: Reward=-2.07, eps=0.100\n",
      "Episode 624: Reward=-1.12, eps=0.100\n",
      "Episode 625: Reward=-2.20, eps=0.100\n",
      "Episode 626: Reward=-1.14, eps=0.100\n",
      "Episode 627: Reward=-1.55, eps=0.100\n",
      "Episode 628: Reward=-1.07, eps=0.100\n",
      "Episode 629: Reward=-2.16, eps=0.100\n",
      "Episode 630: Reward=-0.06, eps=0.100\n",
      "Episode 631: Reward=-1.07, eps=0.100\n",
      "Episode 632: Reward=-2.09, eps=0.100\n",
      "Episode 633: Reward=-0.56, eps=0.100\n",
      "Episode 634: Reward=-5.20, eps=0.100\n",
      "Episode 635: Reward=-2.11, eps=0.100\n",
      "Episode 636: Reward=-2.03, eps=0.100\n",
      "Episode 637: Reward=-1.66, eps=0.100\n",
      "Episode 638: Reward=-5.14, eps=0.100\n",
      "Episode 639: Reward=-2.68, eps=0.100\n",
      "Episode 640: Reward=-2.17, eps=0.100\n",
      "Episode 641: Reward=-1.55, eps=0.100\n",
      "Episode 642: Reward=-1.59, eps=0.100\n",
      "Episode 643: Reward=-3.12, eps=0.100\n",
      "Episode 644: Reward=-2.04, eps=0.100\n",
      "Episode 645: Reward=-4.63, eps=0.100\n",
      "Episode 646: Reward=-6.40, eps=0.100\n",
      "Episode 647: Reward=-1.06, eps=0.100\n",
      "Episode 648: Reward=-6.38, eps=0.100\n",
      "Episode 649: Reward=-1.17, eps=0.100\n",
      "Episode 650: Reward=-1.54, eps=0.100\n",
      "Episode 651: Reward=-1.71, eps=0.100\n",
      "Episode 652: Reward=-3.92, eps=0.100\n",
      "Episode 653: Reward=-1.20, eps=0.100\n",
      "Episode 654: Reward=-2.12, eps=0.100\n",
      "Episode 655: Reward=-1.08, eps=0.100\n",
      "Episode 656: Reward=-0.13, eps=0.100\n",
      "Episode 657: Reward=-0.58, eps=0.100\n",
      "Episode 658: Reward=-14.00, eps=0.100\n",
      "Episode 659: Reward=-6.21, eps=0.100\n",
      "Episode 660: Reward=-1.53, eps=0.100\n",
      "Episode 661: Reward=-3.19, eps=0.100\n",
      "Episode 662: Reward=-2.10, eps=0.100\n",
      "Episode 663: Reward=-1.56, eps=0.100\n",
      "Episode 664: Reward=-3.67, eps=0.100\n",
      "Episode 665: Reward=-0.03, eps=0.100\n",
      "Episode 666: Reward=-4.93, eps=0.100\n",
      "Episode 667: Reward=-3.76, eps=0.100\n",
      "Episode 668: Reward=-1.52, eps=0.100\n",
      "Episode 669: Reward=-2.13, eps=0.100\n",
      "Episode 670: Reward=-1.61, eps=0.100\n",
      "Episode 671: Reward=-4.42, eps=0.100\n",
      "Episode 672: Reward=-0.11, eps=0.100\n",
      "Episode 673: Reward=-0.57, eps=0.100\n",
      "Episode 674: Reward=-0.63, eps=0.100\n",
      "Episode 675: Reward=-1.10, eps=0.100\n",
      "Episode 676: Reward=-1.65, eps=0.100\n",
      "Episode 677: Reward=-2.10, eps=0.100\n",
      "Episode 678: Reward=-3.57, eps=0.100\n",
      "Episode 679: Reward=-2.30, eps=0.100\n",
      "Episode 680: Reward=-0.04, eps=0.100\n",
      "Episode 681: Reward=-1.06, eps=0.100\n",
      "Episode 682: Reward=-2.63, eps=0.100\n",
      "Episode 683: Reward=-3.10, eps=0.100\n",
      "Episode 684: Reward=-2.06, eps=0.100\n",
      "Episode 685: Reward=-6.57, eps=0.100\n",
      "Episode 686: Reward=-3.83, eps=0.100\n",
      "Episode 687: Reward=-3.13, eps=0.100\n",
      "Episode 688: Reward=-2.08, eps=0.100\n",
      "Episode 689: Reward=-7.79, eps=0.100\n",
      "Episode 690: Reward=-1.08, eps=0.100\n",
      "Episode 691: Reward=-1.55, eps=0.100\n",
      "Episode 692: Reward=0.93, eps=0.100\n",
      "Episode 693: Reward=-2.10, eps=0.100\n",
      "Episode 694: Reward=-5.23, eps=0.100\n",
      "Episode 695: Reward=-2.09, eps=0.100\n",
      "Episode 696: Reward=-2.65, eps=0.100\n",
      "Episode 697: Reward=-2.06, eps=0.100\n",
      "Episode 698: Reward=-1.53, eps=0.100\n",
      "Episode 699: Reward=-2.06, eps=0.100\n",
      "Episode 700: Reward=-4.66, eps=0.100\n",
      "Episode 701: Reward=-5.32, eps=0.100\n",
      "Episode 702: Reward=-1.55, eps=0.100\n",
      "Episode 703: Reward=-3.07, eps=0.100\n",
      "Episode 704: Reward=-6.17, eps=0.100\n",
      "Episode 705: Reward=-2.08, eps=0.100\n",
      "Episode 706: Reward=-2.18, eps=0.100\n",
      "Episode 707: Reward=-8.34, eps=0.100\n",
      "Episode 708: Reward=-1.27, eps=0.100\n",
      "Episode 709: Reward=-3.23, eps=0.100\n",
      "Episode 710: Reward=-1.58, eps=0.100\n",
      "Episode 711: Reward=-1.54, eps=0.100\n",
      "Episode 712: Reward=-2.09, eps=0.100\n",
      "Episode 713: Reward=-1.60, eps=0.100\n",
      "Episode 714: Reward=-1.12, eps=0.100\n",
      "Episode 715: Reward=-0.65, eps=0.100\n",
      "Episode 716: Reward=-2.66, eps=0.100\n",
      "Episode 717: Reward=-3.94, eps=0.100\n",
      "Episode 718: Reward=-0.05, eps=0.100\n",
      "Episode 719: Reward=-1.58, eps=0.100\n",
      "Episode 720: Reward=-1.63, eps=0.100\n",
      "Episode 721: Reward=-2.54, eps=0.100\n",
      "Episode 722: Reward=-2.08, eps=0.100\n",
      "Episode 723: Reward=-2.07, eps=0.100\n",
      "Episode 724: Reward=-2.07, eps=0.100\n",
      "Episode 725: Reward=-1.64, eps=0.100\n",
      "Episode 726: Reward=-1.71, eps=0.100\n",
      "Episode 727: Reward=-3.69, eps=0.100\n",
      "Episode 728: Reward=-1.09, eps=0.100\n",
      "Episode 729: Reward=-1.79, eps=0.100\n",
      "Episode 730: Reward=-1.60, eps=0.100\n",
      "Episode 731: Reward=-1.55, eps=0.100\n",
      "Episode 732: Reward=-6.71, eps=0.100\n",
      "Episode 733: Reward=-1.58, eps=0.100\n",
      "Episode 734: Reward=-2.14, eps=0.100\n",
      "Episode 735: Reward=-3.89, eps=0.100\n",
      "Episode 736: Reward=-1.55, eps=0.100\n",
      "Episode 737: Reward=-2.60, eps=0.100\n",
      "Episode 738: Reward=-2.53, eps=0.100\n",
      "Episode 739: Reward=-0.59, eps=0.100\n",
      "Episode 740: Reward=-3.73, eps=0.100\n",
      "Episode 741: Reward=-4.36, eps=0.100\n",
      "Episode 742: Reward=-1.62, eps=0.100\n",
      "Episode 743: Reward=-3.80, eps=0.100\n",
      "Episode 744: Reward=-4.14, eps=0.100\n",
      "Episode 745: Reward=-1.59, eps=0.100\n",
      "Episode 746: Reward=-1.50, eps=0.100\n",
      "Episode 747: Reward=-7.86, eps=0.100\n",
      "Episode 748: Reward=-3.09, eps=0.100\n",
      "Episode 749: Reward=-1.57, eps=0.100\n",
      "Episode 750: Reward=-3.05, eps=0.100\n",
      "Episode 751: Reward=-0.93, eps=0.100\n",
      "Episode 752: Reward=-2.12, eps=0.100\n",
      "Episode 753: Reward=-2.04, eps=0.100\n",
      "Episode 754: Reward=-1.06, eps=0.100\n",
      "Episode 755: Reward=-2.17, eps=0.100\n",
      "Episode 756: Reward=-1.63, eps=0.100\n",
      "Episode 757: Reward=-2.04, eps=0.100\n",
      "Episode 758: Reward=-2.12, eps=0.100\n",
      "Episode 759: Reward=-3.05, eps=0.100\n",
      "Episode 760: Reward=-1.59, eps=0.100\n",
      "Episode 761: Reward=-1.57, eps=0.100\n",
      "Episode 762: Reward=-1.04, eps=0.100\n",
      "Episode 763: Reward=-2.09, eps=0.100\n",
      "Episode 764: Reward=-7.33, eps=0.100\n",
      "Episode 765: Reward=-1.55, eps=0.100\n",
      "Episode 766: Reward=-3.72, eps=0.100\n",
      "Episode 767: Reward=-3.78, eps=0.100\n",
      "Episode 768: Reward=-2.04, eps=0.100\n",
      "Episode 769: Reward=-2.63, eps=0.100\n",
      "Episode 770: Reward=-6.18, eps=0.100\n",
      "Episode 771: Reward=-2.06, eps=0.100\n",
      "Episode 772: Reward=-2.66, eps=0.100\n",
      "Episode 773: Reward=-3.05, eps=0.100\n",
      "Episode 774: Reward=-0.21, eps=0.100\n",
      "Episode 775: Reward=-2.58, eps=0.100\n",
      "Episode 776: Reward=-4.61, eps=0.100\n",
      "Episode 777: Reward=-1.16, eps=0.100\n",
      "Episode 778: Reward=-2.07, eps=0.100\n",
      "Episode 779: Reward=-1.12, eps=0.100\n",
      "Episode 780: Reward=-1.55, eps=0.100\n",
      "Episode 781: Reward=-0.58, eps=0.100\n",
      "Episode 782: Reward=-1.12, eps=0.100\n",
      "Episode 783: Reward=-6.45, eps=0.100\n",
      "Episode 784: Reward=-1.57, eps=0.100\n",
      "Episode 785: Reward=-0.07, eps=0.100\n",
      "Episode 786: Reward=-1.69, eps=0.100\n",
      "Episode 787: Reward=-1.07, eps=0.100\n",
      "Episode 788: Reward=-1.61, eps=0.100\n",
      "Episode 789: Reward=-3.14, eps=0.100\n",
      "Episode 790: Reward=-0.53, eps=0.100\n",
      "Episode 791: Reward=-0.62, eps=0.100\n",
      "Episode 792: Reward=-5.11, eps=0.100\n",
      "Episode 793: Reward=-2.60, eps=0.100\n",
      "Episode 794: Reward=-2.06, eps=0.100\n",
      "Episode 795: Reward=-1.64, eps=0.100\n",
      "Episode 796: Reward=-2.74, eps=0.100\n",
      "Episode 797: Reward=-1.05, eps=0.100\n",
      "Episode 798: Reward=-2.09, eps=0.100\n",
      "Episode 799: Reward=-1.56, eps=0.100\n",
      "Episode 800: Reward=-3.11, eps=0.100\n",
      "Episode 801: Reward=-2.61, eps=0.100\n",
      "Episode 802: Reward=-3.06, eps=0.100\n",
      "Episode 803: Reward=-5.34, eps=0.100\n",
      "Episode 804: Reward=-4.88, eps=0.100\n",
      "Episode 805: Reward=-1.55, eps=0.100\n",
      "Episode 806: Reward=-2.52, eps=0.100\n",
      "Episode 807: Reward=-1.08, eps=0.100\n",
      "Episode 808: Reward=-1.07, eps=0.100\n",
      "Episode 809: Reward=-1.07, eps=0.100\n",
      "Episode 810: Reward=-0.72, eps=0.100\n",
      "Episode 811: Reward=-3.22, eps=0.100\n",
      "Episode 812: Reward=-3.68, eps=0.100\n",
      "Episode 813: Reward=-4.59, eps=0.100\n",
      "Episode 814: Reward=-0.07, eps=0.100\n",
      "Episode 815: Reward=-2.55, eps=0.100\n",
      "Episode 816: Reward=-2.65, eps=0.100\n",
      "Episode 817: Reward=-0.28, eps=0.100\n",
      "Episode 818: Reward=-0.62, eps=0.100\n",
      "Episode 819: Reward=-3.65, eps=0.100\n",
      "Episode 820: Reward=-2.13, eps=0.100\n",
      "Episode 821: Reward=-1.59, eps=0.100\n",
      "Episode 822: Reward=-4.08, eps=0.100\n",
      "Episode 823: Reward=-0.65, eps=0.100\n",
      "Episode 824: Reward=-1.54, eps=0.100\n",
      "Episode 825: Reward=-3.21, eps=0.100\n",
      "Episode 826: Reward=-2.06, eps=0.100\n",
      "Episode 827: Reward=-2.19, eps=0.100\n",
      "Episode 828: Reward=-7.48, eps=0.100\n",
      "Episode 829: Reward=-7.52, eps=0.100\n",
      "Episode 830: Reward=-11.08, eps=0.100\n",
      "Episode 831: Reward=-3.82, eps=0.100\n",
      "Episode 832: Reward=-4.66, eps=0.100\n",
      "Episode 833: Reward=-0.55, eps=0.100\n",
      "Episode 834: Reward=-2.07, eps=0.100\n",
      "Episode 835: Reward=-4.62, eps=0.100\n",
      "Episode 836: Reward=-3.33, eps=0.100\n",
      "Episode 837: Reward=-0.05, eps=0.100\n",
      "Episode 838: Reward=-4.72, eps=0.100\n",
      "Episode 839: Reward=-0.66, eps=0.100\n",
      "Episode 840: Reward=-1.64, eps=0.100\n",
      "Episode 841: Reward=-3.58, eps=0.100\n",
      "Episode 842: Reward=-2.60, eps=0.100\n",
      "Episode 843: Reward=-1.71, eps=0.100\n",
      "Episode 844: Reward=-1.07, eps=0.100\n",
      "Episode 845: Reward=-2.67, eps=0.100\n",
      "Episode 846: Reward=-0.52, eps=0.100\n",
      "Episode 847: Reward=-2.61, eps=0.100\n",
      "Episode 848: Reward=-2.17, eps=0.100\n",
      "Episode 849: Reward=-2.12, eps=0.100\n",
      "Episode 850: Reward=-2.56, eps=0.100\n",
      "Episode 851: Reward=-0.56, eps=0.100\n",
      "Episode 852: Reward=-0.57, eps=0.100\n",
      "Episode 853: Reward=-1.58, eps=0.100\n",
      "Episode 854: Reward=-2.05, eps=0.100\n",
      "Episode 855: Reward=-3.52, eps=0.100\n",
      "Episode 856: Reward=-5.36, eps=0.100\n",
      "Episode 857: Reward=-2.55, eps=0.100\n",
      "Episode 858: Reward=-2.63, eps=0.100\n",
      "Episode 859: Reward=-5.80, eps=0.100\n",
      "Episode 860: Reward=-3.81, eps=0.100\n",
      "Episode 861: Reward=-4.86, eps=0.100\n",
      "Episode 862: Reward=-1.59, eps=0.100\n",
      "Episode 863: Reward=-4.14, eps=0.100\n",
      "Episode 864: Reward=-2.12, eps=0.100\n",
      "Episode 865: Reward=-1.12, eps=0.100\n",
      "Episode 866: Reward=-2.57, eps=0.100\n",
      "Episode 867: Reward=-1.06, eps=0.100\n",
      "Episode 868: Reward=-4.76, eps=0.100\n",
      "Episode 869: Reward=-3.90, eps=0.100\n",
      "Episode 870: Reward=-1.57, eps=0.100\n",
      "Episode 871: Reward=-0.05, eps=0.100\n",
      "Episode 872: Reward=-1.63, eps=0.100\n",
      "Episode 873: Reward=-4.55, eps=0.100\n",
      "Episode 874: Reward=-2.04, eps=0.100\n",
      "Episode 875: Reward=-2.54, eps=0.100\n",
      "Episode 876: Reward=-1.68, eps=0.100\n",
      "Episode 877: Reward=-3.64, eps=0.100\n",
      "Episode 878: Reward=-2.04, eps=0.100\n",
      "Episode 879: Reward=-4.16, eps=0.100\n",
      "Episode 880: Reward=-4.09, eps=0.100\n",
      "Episode 881: Reward=-1.12, eps=0.100\n",
      "Episode 882: Reward=-1.03, eps=0.100\n",
      "Episode 883: Reward=-2.13, eps=0.100\n",
      "Episode 884: Reward=-3.14, eps=0.100\n",
      "Episode 885: Reward=-1.57, eps=0.100\n",
      "Episode 886: Reward=-2.56, eps=0.100\n",
      "Episode 887: Reward=-1.16, eps=0.100\n",
      "Episode 888: Reward=-4.43, eps=0.100\n",
      "Episode 889: Reward=-3.04, eps=0.100\n",
      "Episode 890: Reward=-3.17, eps=0.100\n",
      "Episode 891: Reward=-3.69, eps=0.100\n",
      "Episode 892: Reward=-1.57, eps=0.100\n",
      "Episode 893: Reward=-0.57, eps=0.100\n",
      "Episode 894: Reward=-2.06, eps=0.100\n",
      "Episode 895: Reward=-2.71, eps=0.100\n",
      "Episode 896: Reward=-3.07, eps=0.100\n",
      "Episode 897: Reward=-1.05, eps=0.100\n",
      "Episode 898: Reward=-2.51, eps=0.100\n",
      "Episode 899: Reward=-6.95, eps=0.100\n",
      "Episode 900: Reward=-1.06, eps=0.100\n",
      "Episode 901: Reward=-3.12, eps=0.100\n",
      "Episode 902: Reward=0.95, eps=0.100\n",
      "Episode 903: Reward=-5.18, eps=0.100\n",
      "Episode 904: Reward=-4.81, eps=0.100\n",
      "Episode 905: Reward=-1.57, eps=0.100\n",
      "Episode 906: Reward=-1.12, eps=0.100\n",
      "Episode 907: Reward=-1.19, eps=0.100\n",
      "Episode 908: Reward=-2.54, eps=0.100\n",
      "Episode 909: Reward=-1.62, eps=0.100\n",
      "Episode 910: Reward=-2.88, eps=0.100\n",
      "Episode 911: Reward=-2.54, eps=0.100\n",
      "Episode 912: Reward=-3.15, eps=0.100\n",
      "Episode 913: Reward=-2.17, eps=0.100\n",
      "Episode 914: Reward=-1.07, eps=0.100\n",
      "Episode 915: Reward=-11.28, eps=0.100\n",
      "Episode 916: Reward=-2.70, eps=0.100\n",
      "Episode 917: Reward=-0.54, eps=0.100\n",
      "Episode 918: Reward=-2.61, eps=0.100\n",
      "Episode 919: Reward=-3.66, eps=0.100\n",
      "Episode 920: Reward=-5.42, eps=0.100\n",
      "Episode 921: Reward=-2.58, eps=0.100\n",
      "Episode 922: Reward=-2.76, eps=0.100\n",
      "Episode 923: Reward=-1.55, eps=0.100\n",
      "Episode 924: Reward=-2.55, eps=0.100\n",
      "Episode 925: Reward=-0.05, eps=0.100\n",
      "Episode 926: Reward=-2.53, eps=0.100\n",
      "Episode 927: Reward=-3.27, eps=0.100\n",
      "Episode 928: Reward=-1.31, eps=0.100\n",
      "Episode 929: Reward=-0.56, eps=0.100\n",
      "Episode 930: Reward=-3.05, eps=0.100\n",
      "Episode 931: Reward=-3.75, eps=0.100\n",
      "Episode 932: Reward=-2.86, eps=0.100\n",
      "Episode 933: Reward=-3.09, eps=0.100\n",
      "Episode 934: Reward=-2.60, eps=0.100\n",
      "Episode 935: Reward=-2.20, eps=0.100\n",
      "Episode 936: Reward=-4.23, eps=0.100\n",
      "Episode 937: Reward=-3.26, eps=0.100\n",
      "Episode 938: Reward=-2.59, eps=0.100\n",
      "Episode 939: Reward=-1.04, eps=0.100\n",
      "Episode 940: Reward=-0.05, eps=0.100\n",
      "Episode 941: Reward=-1.87, eps=0.100\n",
      "Episode 942: Reward=-2.76, eps=0.100\n",
      "Episode 943: Reward=-0.73, eps=0.100\n",
      "Episode 944: Reward=-1.04, eps=0.100\n",
      "Episode 945: Reward=-3.26, eps=0.100\n",
      "Episode 946: Reward=-1.57, eps=0.100\n",
      "Episode 947: Reward=-4.79, eps=0.100\n",
      "Episode 948: Reward=-0.18, eps=0.100\n",
      "Episode 949: Reward=-2.69, eps=0.100\n",
      "Episode 950: Reward=-3.17, eps=0.100\n",
      "Episode 951: Reward=-0.09, eps=0.100\n",
      "Episode 952: Reward=-2.16, eps=0.100\n",
      "Episode 953: Reward=-1.57, eps=0.100\n",
      "Episode 954: Reward=-2.69, eps=0.100\n",
      "Episode 955: Reward=-2.11, eps=0.100\n",
      "Episode 956: Reward=-4.85, eps=0.100\n",
      "Episode 957: Reward=-2.17, eps=0.100\n",
      "Episode 958: Reward=-2.06, eps=0.100\n",
      "Episode 959: Reward=-2.11, eps=0.100\n",
      "Episode 960: Reward=-3.14, eps=0.100\n",
      "Episode 961: Reward=-0.04, eps=0.100\n",
      "Episode 962: Reward=-1.62, eps=0.100\n",
      "Episode 963: Reward=-4.82, eps=0.100\n",
      "Episode 964: Reward=-0.74, eps=0.100\n",
      "Episode 965: Reward=-4.79, eps=0.100\n",
      "Episode 966: Reward=-1.23, eps=0.100\n",
      "Episode 967: Reward=-3.65, eps=0.100\n",
      "Episode 968: Reward=-2.06, eps=0.100\n",
      "Episode 969: Reward=-2.04, eps=0.100\n",
      "Episode 970: Reward=-5.48, eps=0.100\n",
      "Episode 971: Reward=-2.07, eps=0.100\n",
      "Episode 972: Reward=-1.05, eps=0.100\n",
      "Episode 973: Reward=-1.10, eps=0.100\n",
      "Episode 974: Reward=-2.74, eps=0.100\n",
      "Episode 975: Reward=-2.15, eps=0.100\n",
      "Episode 976: Reward=-1.78, eps=0.100\n",
      "Episode 977: Reward=-5.74, eps=0.100\n",
      "Episode 978: Reward=-2.64, eps=0.100\n",
      "Episode 979: Reward=-1.57, eps=0.100\n",
      "Episode 980: Reward=-1.56, eps=0.100\n",
      "Episode 981: Reward=-2.04, eps=0.100\n",
      "Episode 982: Reward=-1.08, eps=0.100\n",
      "Episode 983: Reward=-2.14, eps=0.100\n",
      "Episode 984: Reward=-2.28, eps=0.100\n",
      "Episode 985: Reward=-4.94, eps=0.100\n",
      "Episode 986: Reward=-2.61, eps=0.100\n",
      "Episode 987: Reward=-1.59, eps=0.100\n",
      "Episode 988: Reward=-3.18, eps=0.100\n",
      "Episode 989: Reward=-1.06, eps=0.100\n",
      "Episode 990: Reward=-2.08, eps=0.100\n",
      "Episode 991: Reward=-1.62, eps=0.100\n",
      "Episode 992: Reward=-1.08, eps=0.100\n",
      "Episode 993: Reward=-4.26, eps=0.100\n",
      "Episode 994: Reward=-3.37, eps=0.100\n",
      "Episode 995: Reward=-2.03, eps=0.100\n",
      "Episode 996: Reward=-1.08, eps=0.100\n",
      "Episode 997: Reward=-3.92, eps=0.100\n",
      "Episode 998: Reward=-4.17, eps=0.100\n",
      "Episode 999: Reward=-2.06, eps=0.100\n",
      "Episode 1000: Reward=-1.57, eps=0.100\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement training logic for Snake Game here\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buf = deque(maxlen=capacity)\n",
    "    def push(self, s,a,r,s2,done):\n",
    "        self.buf.append((s,a,r,s2,done))\n",
    "    def sample(self, bs):\n",
    "        batch = random.sample(self.buf, bs)\n",
    "        ss, aa, rr, ss2, dd = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(ss, dtype=torch.float32),\n",
    "            torch.tensor(aa, dtype=torch.int64),\n",
    "            torch.tensor(rr, dtype=torch.float32),\n",
    "            torch.tensor(ss2, dtype=torch.float32),\n",
    "            torch.tensor(dd, dtype=torch.bool)\n",
    "        )\n",
    "    def __len__(self): return len(self.buf)\n",
    "\n",
    "class SnakeAgent:\n",
    "    def __init__(self, env, device='cpu'):\n",
    "        self.env = env\n",
    "        obs_shape = env.observation_space.shape  # (H,W)\n",
    "        input_dim = obs_shape[0]*obs_shape[1]\n",
    "        n_actions = env.action_space.n\n",
    "        self.online_net = DQN(input_dim, n_actions).to(device)\n",
    "        self.target_net = DQN(input_dim, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=1e-3)\n",
    "        self.replay = ReplayBuffer(10000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.eps, self.eps_end, self.eps_dec = 1.0, 0.1, 1e-4\n",
    "        self.step_count = 0\n",
    "        self.target_update = 1000\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.eps:\n",
    "            return self.env.action_space.sample()\n",
    "        with torch.no_grad():\n",
    "            st_v = torch.tensor(state[None,:], dtype=torch.float32).to(self.device)\n",
    "            q_vals = self.online_net(st_v)\n",
    "            return int(q_vals.argmax(dim=1).item())\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay) < self.batch_size:\n",
    "            return\n",
    "        s, a, r, s2, d = self.replay.sample(self.batch_size)\n",
    "        s  = s.to(self.device)\n",
    "        s2 = s2.to(self.device)\n",
    "        a  = a.to(self.device)\n",
    "        r  = r.to(self.device)\n",
    "        d  = d.to(self.device)\n",
    "\n",
    "        q = self.online_net(s)\n",
    "        q_s_a = q.gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_next = self.target_net(s2)\n",
    "            q_next_max = q_next.max(dim=1).values\n",
    "            q_next_max[d] = 0.0\n",
    "        q_target = r + self.gamma * q_next_max\n",
    "\n",
    "        loss = F.mse_loss(q_s_a, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # eps decay\n",
    "        self.eps = max(self.eps_end, self.eps - self.eps_dec)\n",
    "        # target update\n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "    def train(self, episodes=500):\n",
    "        for ep in range(1, episodes+1):\n",
    "            state, _ = self.env.reset()\n",
    "            total_r = 0.0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                nxt, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.replay.push(state, action, reward, nxt, done)\n",
    "                self.train_step()\n",
    "                state = nxt\n",
    "                total_r += reward\n",
    "                self.step_count += 1\n",
    "            print(f\"Episode {ep}: Reward={total_r:.2f}, eps={self.eps:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env    = SnakeGame(size=10, render_mode=None)\n",
    "    agent  = SnakeAgent(env, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    agent.train(episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0e10565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_snake_model(model, size=20, episodes=10, render=True):\n",
    "    env = SnakeGame(size=size, render_mode=\"human\" if render else None)\n",
    "    model.eval()\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "    \n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards) / episodes\n",
    "\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a7eb581",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x400 and 100x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Run evaluation for Snake Game here\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_snake_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m, in \u001b[0;36mevaluate_snake_model\u001b[0;34m(model, size, episodes, render)\u001b[0m\n\u001b[1;32m     13\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/Desktop/Summer25/SOC25-BOB/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer25/SOC25-BOB/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer25/SOC25-BOB/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer25/SOC25-BOB/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Summer25/SOC25-BOB/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Summer25/SOC25-BOB/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer25/SOC25-BOB/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Summer25/SOC25-BOB/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x400 and 100x128)"
     ]
    }
   ],
   "source": [
    "# TODO: Run evaluation for Snake Game here\n",
    "evaluate_snake_model(agent.online_net, size=20, episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChaseEscapeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dt = 0.1\n",
    "        self.max_speed = 0.4\n",
    "        self.agent_radius = 0.05\n",
    "        self.target_radius = 0.05\n",
    "        self.chaser_radius = 0.07\n",
    "        self.chaser_speed = 0.03\n",
    "\n",
    "        self.action_space = gym.spaces.MultiDiscrete([3, 3])  # actions in {0,1,2} map to [-1,0,1]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=(8,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.screen_size = 500\n",
    "        self.np_random = None\n",
    "\n",
    "        if render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "    def sample_pos(self, far_from=None, min_dist=0.5):\n",
    "        while True:\n",
    "            pos = self.np_random.uniform(low=-0.8, high=0.8, size=(2,))\n",
    "            if far_from is None or np.linalg.norm(pos - far_from) >= min_dist:\n",
    "                return pos\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.agent_pos = self.sample_pos()\n",
    "        self.agent_vel = np.zeros(2, dtype=np.float32)\n",
    "        self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
    "        self.chaser_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.7)\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # TODO: Decide how to pass the state (don't use pixel values)\n",
    "        pass\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # TODO: Add reward scheme\n",
    "        # 1) Try to make the agent stay within bounds\n",
    "        # 2) The agent shouldn't idle around\n",
    "        # 3) The agent should go for the reward\n",
    "        # 4) The agent should avoid the chaser\n",
    "\n",
    "        accel = (np.array(action) - 1) * 0.1\n",
    "        self.agent_vel += accel\n",
    "        self.agent_vel = np.clip(self.agent_vel, -self.max_speed, self.max_speed)\n",
    "        self.agent_pos += self.agent_vel * self.dt\n",
    "        self.agent_pos = np.clip(self.agent_pos, -1, 1)\n",
    "\n",
    "        direction = self.agent_pos - self.chaser_pos\n",
    "        norm = np.linalg.norm(direction)\n",
    "        if norm > 1e-5:\n",
    "            self.chaser_pos += self.chaser_speed * direction / norm\n",
    "\n",
    "        dist_to_target = np.linalg.norm(self.agent_pos - self.target_pos)\n",
    "        dist_to_chaser = np.linalg.norm(self.agent_pos - self.chaser_pos)\n",
    "\n",
    "        reward = 0.0\n",
    "        terminated = False\n",
    "\n",
    "        if dist_to_target < self.agent_radius + self.target_radius:\n",
    "            self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
    "\n",
    "        if dist_to_chaser < self.agent_radius + self.chaser_radius:\n",
    "            terminated = True\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, self._get_info()\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.close()\n",
    "\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        def to_screen(p):\n",
    "            x = int((p[0] + 1) / 2 * self.screen_size)\n",
    "            y = int((1 - (p[1] + 1) / 2) * self.screen_size)\n",
    "            return x, y\n",
    "\n",
    "        pygame.draw.circle(self.screen, (0, 255, 0), to_screen(self.target_pos), int(self.target_radius * self.screen_size))\n",
    "        pygame.draw.circle(self.screen, (0, 0, 255), to_screen(self.agent_pos), int(self.agent_radius * self.screen_size))\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), to_screen(self.chaser_pos), int(self.chaser_radius * self.screen_size))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train and evaluate CatMouseEnv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
